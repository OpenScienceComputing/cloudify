{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5672d",
   "metadata": {},
   "source": "# Build a Static STAC Catalog and Browse with STAC Browser\n\nThis notebook builds STAC items for three public icechunk stores, assembles\nthem into a static STAC catalog, saves it **locally**, uploads it to S3 with\nfsspec, and produces a [STAC Browser](https://github.com/radiantearth/stac-browser)\nURL for immediate browsing — no server required.\n\n## Architecture\n\n```\ns3://BUCKET/PREFIX/\n  catalog.json\n  noaa-gfs-forecast-{snapshot}/noaa-gfs-forecast-{snapshot}.json\n  noaa-hrrr-forecast-48-hour-{snapshot}/noaa-hrrr-forecast-48-hour-{snapshot}.json\n  nldas-3-virtual-zarr-{snapshot}/nldas-3-virtual-zarr-{snapshot}.json\n```\n\nThe catalog uses `SELF_CONTAINED` relative links so it can be moved between\nbuckets/prefixes without breaking.  STAC Browser is a client-side Vue.js app\nhosted by Radiant Earth — it fetches and renders the catalog directly from S3\nwith no backend.\n\n## Install dependencies\n```\npip install icechunk xarray zarr pystac xstac rioxarray s3fs cloudify\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb009c",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport tempfile\nimport warnings\nfrom pathlib import Path\n\nimport icechunk\nimport pystac\nimport rioxarray\nimport s3fs\nimport xarray as xr\n\nfrom cloudify.stac import build_stac_item_from_icechunk\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n    category=UserWarning,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "ebe9262e",
   "metadata": {},
   "source": "## 1. Configure the output catalog location\n\n`CATALOG_BUCKET` and `CATALOG_PREFIX` are set in **Step 6** just before the upload,\nso you can inspect the local JSON first.  Set up your AWS credentials in the\nenvironment before running Step 6 (env vars, `~/.aws/credentials`, or instance profile)."
  },
  {
   "cell_type": "markdown",
   "id": "0cc98dc7",
   "metadata": {},
   "source": "## 2. Configure local staging directory\n\nThe catalog is first written to a local temp directory, then uploaded to S3.\nThis lets you inspect the JSON before it goes live and avoids partial writes\non the remote."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95dd57",
   "metadata": {},
   "outputs": [],
   "source": "LOCAL_CATALOG_DIR = Path(tempfile.mkdtemp(prefix=\"stac-catalog-\"))\nprint(f\"Local staging directory: {LOCAL_CATALOG_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "f99ff7cc",
   "metadata": {},
   "source": [
    "## 3. Build STAC items for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_icechunk(bucket, prefix, region, anonymous=True,\n",
    "                  virtual_source=None, snapshot_id=None, branch=\"main\"):\n",
    "    \"\"\"Open an icechunk repo and return (session, ds).\"\"\"\n",
    "    storage = icechunk.s3_storage(\n",
    "        bucket=bucket, prefix=prefix, region=region, anonymous=anonymous\n",
    "    )\n",
    "    config = icechunk.RepositoryConfig.default()\n",
    "    repo_kwargs = {\"storage\": storage, \"config\": config}\n",
    "\n",
    "    if virtual_source:\n",
    "        config.set_virtual_chunk_container(\n",
    "            icechunk.VirtualChunkContainer(\n",
    "                virtual_source, icechunk.s3_store(region=region)\n",
    "            )\n",
    "        )\n",
    "        repo_kwargs[\"authorize_virtual_chunk_access\"] = icechunk.containers_credentials(\n",
    "            {virtual_source: icechunk.s3_anonymous_credentials()}\n",
    "        )\n",
    "\n",
    "    repo = icechunk.Repository.open(**repo_kwargs)\n",
    "    session = repo.readonly_session(\n",
    "        snapshot_id=snapshot_id if snapshot_id else branch\n",
    "    ) if snapshot_id else repo.readonly_session(branch=branch)\n",
    "    ds = xr.open_zarr(session.store, chunks=None, consolidated=False, zarr_format=3)\n",
    "    return session, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b21465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Opening GFS...\")\n",
    "gfs_session, gfs_ds = open_icechunk(\n",
    "    bucket=\"dynamical-noaa-gfs\",\n",
    "    prefix=\"noaa-gfs-forecast/v0.2.7.icechunk/\",\n",
    "    region=\"us-west-2\",\n",
    ")\n",
    "print(f\"  snapshot: {gfs_session.snapshot_id}  dims: {dict(gfs_ds.sizes)}\")\n",
    "\n",
    "print(\"Opening HRRR...\")\n",
    "hrrr_session, hrrr_ds = open_icechunk(\n",
    "    bucket=\"dynamical-noaa-hrrr\",\n",
    "    prefix=\"noaa-hrrr-forecast-48-hour/v0.1.0.icechunk/\",\n",
    "    region=\"us-west-2\",\n",
    ")\n",
    "print(f\"  snapshot: {hrrr_session.snapshot_id}  dims: {dict(hrrr_ds.sizes)}\")\n",
    "\n",
    "NLDAS_SNAPSHOT = \"YTNGFY4WY9189GEH1FNG\"\n",
    "NLDAS_VIRTUAL_SOURCE = \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\"\n",
    "print(\"Opening NLDAS-3...\")\n",
    "nldas_session, nldas_ds = open_icechunk(\n",
    "    bucket=\"nasa-waterinsight\",\n",
    "    prefix=\"virtual-zarr-store/NLDAS-3-icechunk/\",\n",
    "    region=\"us-west-2\",\n",
    "    virtual_source=NLDAS_VIRTUAL_SOURCE,\n",
    "    snapshot_id=NLDAS_SNAPSHOT,\n",
    ")\n",
    "print(f\"  snapshot: {nldas_session.snapshot_id}  dims: {dict(nldas_ds.sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb051fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "\n",
    "gfs_snap = gfs_session.snapshot_id\n",
    "hrrr_snap = hrrr_session.snapshot_id\n",
    "\n",
    "gfs_item_dict = build_stac_item_from_icechunk(\n",
    "    gfs_ds,\n",
    "    item_id=f\"noaa-gfs-forecast-{gfs_snap.lower()}\",\n",
    "    icechunk_href=\"s3://dynamical-noaa-gfs/noaa-gfs-forecast/v0.2.7.icechunk/\",\n",
    "    snapshot_id=gfs_snap,\n",
    "    storage_schemes={\"aws-s3-dynamical-noaa-gfs\": {\n",
    "        \"type\": \"aws-s3\", \"bucket\": \"dynamical-noaa-gfs\",\n",
    "        \"region\": \"us-west-2\", \"anonymous\": True,\n",
    "    }},\n",
    "    title=\"NOAA GFS Forecast (dynamical.org)\",\n",
    "    providers=[pystac.Provider(name=\"dynamical.org\", roles=[\"producer\",\"processor\",\"host\"], url=\"https://dynamical.org\")],\n",
    "    virtual=False,\n",
    "    temporal_dimension=\"init_time\", x_dimension=\"longitude\", y_dimension=\"latitude\",\n",
    ")\n",
    "\n",
    "hrrr_crs = hrrr_ds.rio.crs\n",
    "hrrr_item_dict = build_stac_item_from_icechunk(\n",
    "    hrrr_ds,\n",
    "    item_id=f\"noaa-hrrr-forecast-48-hour-{hrrr_snap.lower()}\",\n",
    "    icechunk_href=\"s3://dynamical-noaa-hrrr/noaa-hrrr-forecast-48-hour/v0.1.0.icechunk/\",\n",
    "    snapshot_id=hrrr_snap,\n",
    "    storage_schemes={\"aws-s3-dynamical-noaa-hrrr\": {\n",
    "        \"type\": \"aws-s3\", \"bucket\": \"dynamical-noaa-hrrr\",\n",
    "        \"region\": \"us-west-2\", \"anonymous\": True,\n",
    "    }},\n",
    "    title=\"NOAA HRRR 48-Hour Forecast (dynamical.org)\",\n",
    "    providers=[pystac.Provider(name=\"dynamical.org\", roles=[\"producer\",\"processor\",\"host\"], url=\"https://dynamical.org\")],\n",
    "    virtual=False,\n",
    "    temporal_dimension=\"init_time\", x_dimension=\"x\", y_dimension=\"y\",\n",
    "    reference_system=hrrr_crs.to_epsg() or hrrr_crs.to_wkt(),\n",
    ")\n",
    "\n",
    "nldas_item_dict = build_stac_item_from_icechunk(\n",
    "    nldas_ds,\n",
    "    item_id=f\"nldas-3-virtual-zarr-{NLDAS_SNAPSHOT.lower()}\",\n",
    "    icechunk_href=\"s3://nasa-waterinsight/virtual-zarr-store/NLDAS-3-icechunk/\",\n",
    "    snapshot_id=NLDAS_SNAPSHOT,\n",
    "    storage_schemes={\"aws-s3-nasa-waterinsight\": {\n",
    "        \"type\": \"aws-s3\", \"bucket\": \"nasa-waterinsight\",\n",
    "        \"region\": \"us-west-2\", \"anonymous\": True,\n",
    "    }},\n",
    "    title=\"NLDAS-3 Virtual Zarr Store\",\n",
    "    providers=[pystac.Provider(name=\"NLDAS\", roles=[\"producer\",\"processor\",\"licensor\"], url=\"https://ldas.gsfc.nasa.gov/nldas\")],\n",
    "    virtual=True,\n",
    "    virtual_hrefs=[NLDAS_VIRTUAL_SOURCE],\n",
    "    temporal_dimension=\"time\", x_dimension=\"lon\", y_dimension=\"lat\",\n",
    ")\n",
    "\n",
    "print(\"Items built:\")\n",
    "for d in [gfs_item_dict, hrrr_item_dict, nldas_item_dict]:\n",
    "    print(f\"  {d['id']}  bbox={d['bbox']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04d20e",
   "metadata": {},
   "source": [
    "## 4. Assemble the pystac Catalog\n",
    "\n",
    "We use `SELF_CONTAINED` so all links are relative — the catalog can be\n",
    "moved between buckets/prefixes without breaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f94e85",
   "metadata": {},
   "outputs": [],
   "source": "catalog = pystac.Catalog(\n    id=\"weather-forecast-icechunk\",\n    description=(\n        \"Public weather forecast datasets stored as Icechunk repositories on AWS S3. \"\n        \"All items can be opened directly with xarray via xpystac.\"\n    ),\n    catalog_type=pystac.CatalogType.SELF_CONTAINED,\n)\n\nfor item_dict in [gfs_item_dict, hrrr_item_dict, nldas_item_dict]:\n    # from_dict preserves top-level extra_fields (e.g. storage:schemes)\n    catalog.add_item(pystac.Item.from_dict(item_dict))\n\nprint(f\"Catalog assembled with {len(list(catalog.get_items()))} items.\")"
  },
  {
   "cell_type": "markdown",
   "id": "753f0089",
   "metadata": {},
   "source": "## 5. Save catalog locally\n\nWrite all JSON files to the local staging directory for inspection before upload."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71379d6c",
   "metadata": {},
   "outputs": [],
   "source": "catalog.normalize_hrefs(str(LOCAL_CATALOG_DIR))\ncatalog.save()\n\nprint(f\"Catalog saved locally to: {LOCAL_CATALOG_DIR}\")\nprint()\nfor f in sorted(LOCAL_CATALOG_DIR.rglob(\"*.json\")):\n    rel = f.relative_to(LOCAL_CATALOG_DIR)\n    size = f.stat().st_size\n    print(f\"  {rel}  ({size:,} bytes)\")"
  },
  {
   "cell_type": "markdown",
   "id": "916527ca",
   "metadata": {},
   "source": "## 6. Upload to S3\n\nUpload all local JSON files to S3, preserving the directory structure.\nSet `CATALOG_BUCKET` and `CATALOG_PREFIX` to a publicly-readable S3 location\nyou have write access to.  The bucket must allow public `s3:GetObject` so\nSTAC Browser can fetch the files from the browser.\n\nAWS credentials are read from the environment (env vars, `~/.aws/credentials`,\nor an instance profile).  For a public-read upload, set the bucket ACL or\nbucket policy separately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107c610",
   "metadata": {},
   "outputs": [],
   "source": "CATALOG_BUCKET = \"my-public-bucket\"       # ← change me\nCATALOG_PREFIX = \"stac/weather-forecasts\"  # ← change me\n\nCATALOG_ROOT_HTTPS = f\"https://{CATALOG_BUCKET}.s3.amazonaws.com/{CATALOG_PREFIX}\"\n\nfs = s3fs.S3FileSystem()   # reads credentials from env / ~/.aws/credentials\n\nprint(f\"Uploading to s3://{CATALOG_BUCKET}/{CATALOG_PREFIX} ...\")\nfor local_file in sorted(LOCAL_CATALOG_DIR.rglob(\"*.json\")):\n    rel = local_file.relative_to(LOCAL_CATALOG_DIR)\n    s3_dest = f\"{CATALOG_BUCKET}/{CATALOG_PREFIX}/{rel}\"\n    fs.put(str(local_file), s3_dest)\n    print(f\"  {rel}  →  s3://{s3_dest}\")\n\nprint(\"\\nUpload complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "953c3486",
   "metadata": {},
   "source": "## 7. Browse with STAC Browser\n\nThe [Radiant Earth hosted STAC Browser](https://radiantearth.github.io/stac-browser/)\nis a fully client-side app — paste any public STAC catalog URL and it\nrenders it in the browser with no backend required."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9962f14",
   "metadata": {},
   "outputs": [],
   "source": "catalog_url = f\"{CATALOG_ROOT_HTTPS}/catalog.json\"\nbrowser_url = f\"https://radiantearth.github.io/stac-browser/#/external/{catalog_url}\"\n\nprint(\"Catalog URL:\")\nprint(f\"  {catalog_url}\")\nprint()\nprint(\"STAC Browser URL:\")\nprint(f\"  {browser_url}\")\n\nfrom IPython.display import display, Markdown\ndisplay(Markdown(f\"[Open in STAC Browser]({browser_url})\"))"
  },
  {
   "cell_type": "markdown",
   "id": "jpai8e947v",
   "source": "## 8. Verify round-trip from catalog\n\nReload the catalog from S3 via its public HTTPS URL and confirm each item\ncan be opened with xpystac.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tntbj7gs1e",
   "source": "import xpystac  # noqa: F401 — registers xarray backend\n\nloaded_catalog = pystac.Catalog.from_file(catalog_url)\n\nfor item in loaded_catalog.get_items():\n    asset_key = next(k for k in item.assets if \"@\" in k)\n    asset = item.assets[asset_key]\n    ds = xr.open_dataset(asset)\n    print(f\"{item.id}\")\n    print(f\"  dims:  {dict(ds.sizes)}\")\n    print(f\"  bbox:  {item.bbox}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}