{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a STAC Item for the NLDAS-3 Virtual Icechunk Store\n",
    "\n",
    "This notebook shows how to use `cloudify.stac` to generate a STAC item for the\n",
    "NLDAS-3 meteorological forcing dataset stored as a **virtual** Icechunk repository\n",
    "on AWS S3.\n",
    "\n",
    "NLDAS-3 differs from the GFS and HRRR examples in two important ways:\n",
    "\n",
    "1. **Virtual chunks** — the Icechunk repo holds only chunk *references*; the actual\n",
    "   data bytes live in the original source files at\n",
    "   `s3://nasa-waterinsight/NLDAS3/forcing/daily/`.  Opening the store therefore\n",
    "   requires a `VirtualChunkContainer` config and anonymous credentials for the\n",
    "   source bucket.  Pass `virtual=True` so the asset roles reflect this.\n",
    "\n",
    "2. **Snapshot-pinned session** — rather than opening the `main` branch, we pin to\n",
    "   a specific `snapshot_id` for reproducibility.  The snapshot ID is also used as\n",
    "   part of the STAC item ID and asset key.\n",
    "\n",
    "> **Note on the `storage:schemes` bug**: the original notebook that built this item\n",
    "> placed `storage:schemes` inside `item.properties`, which causes\n",
    "> `xr.open_dataset(asset)` via xpystac to fail with `KeyError: 'storage:schemes'`.\n",
    "> `build_stac_item_from_icechunk` always places it in `item.extra_fields`\n",
    "> (top-level), which is where xpystac looks.\n",
    "\n",
    "## Install dependencies\n",
    "```\n",
    "pip install icechunk xarray zarr pystac xstac rioxarray cloudify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import icechunk\n",
    "import pystac\n",
    "import rioxarray  # registers .rio accessor for CRS-aware bbox\n",
    "import xarray as xr\n",
    "\n",
    "from cloudify.stac import build_stac_item_from_icechunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Open the virtual Icechunk store\n",
    "\n",
    "Opening a virtual store requires two extra steps compared to a regular Icechunk store:\n",
    "\n",
    "- A `VirtualChunkContainer` that tells the library where the source chunk data lives\n",
    "- `containers_credentials` granting access to the virtual chunk source bucket\n",
    "\n",
    "Both are needed at open time, but they don't affect the STAC item — they're an\n",
    "implementation detail of how the data is read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"nasa-waterinsight\"\n",
    "PREFIX = \"virtual-zarr-store/NLDAS-3-icechunk/\"\n",
    "REGION = \"us-west-2\"\n",
    "ICECHUNK_HREF = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "\n",
    "# Snapshot to pin to (from original dataset notebook)\n",
    "SNAPSHOT_ID = \"YTNGFY4WY9189GEH1FNG\"\n",
    "\n",
    "# Virtual chunk source — where the actual data bytes live\n",
    "VIRTUAL_SOURCE = \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\"\n",
    "\n",
    "storage = icechunk.s3_storage(\n",
    "    bucket=BUCKET,\n",
    "    prefix=PREFIX,\n",
    "    region=REGION,\n",
    "    anonymous=True,\n",
    ")\n",
    "\n",
    "config = icechunk.RepositoryConfig.default()\n",
    "config.set_virtual_chunk_container(\n",
    "    icechunk.VirtualChunkContainer(\n",
    "        VIRTUAL_SOURCE,\n",
    "        icechunk.s3_store(region=REGION),\n",
    "    )\n",
    ")\n",
    "\n",
    "virtual_credentials = icechunk.containers_credentials(\n",
    "    {VIRTUAL_SOURCE: icechunk.s3_anonymous_credentials()}\n",
    ")\n",
    "\n",
    "repo = icechunk.Repository.open(\n",
    "    storage=storage,\n",
    "    config=config,\n",
    "    authorize_virtual_chunk_access=virtual_credentials,\n",
    ")\n",
    "\n",
    "session = repo.readonly_session(snapshot_id=SNAPSHOT_ID)\n",
    "print(f\"snapshot_id: {session.snapshot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "ds = xr.open_zarr(session.store, consolidated=False, zarr_format=3)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define storage schemes and providers\n",
    "\n",
    "Both the Icechunk repo and the virtual chunk source live in the same S3 bucket\n",
    "(`nasa-waterinsight`), so a single storage scheme entry covers both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_schemes = {\n",
    "    \"aws-s3-nasa-waterinsight\": {\n",
    "        \"type\": \"aws-s3\",\n",
    "        \"platform\": \"https://{bucket}.s3.{region}.amazonaws.com\",\n",
    "        \"bucket\": BUCKET,\n",
    "        \"region\": REGION,\n",
    "        \"anonymous\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "providers = [\n",
    "    pystac.Provider(\n",
    "        name=\"NLDAS\",\n",
    "        description=\"NASA Land Data Assimilation Systems\",\n",
    "        roles=[\"producer\", \"processor\", \"licensor\"],\n",
    "        url=\"https://ldas.gsfc.nasa.gov/nldas\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the STAC item\n",
    "\n",
    "Key arguments for NLDAS-3 that differ from the GFS/HRRR examples:\n",
    "\n",
    "| Argument | Value | Reason |\n",
    "|---|---|---|\n",
    "| `virtual` | `True` | Chunks are virtual references, not embedded data |\n",
    "| `temporal_dimension` | `\"time\"` | Standard time dimension (not `init_time`) |\n",
    "| `x_dimension` | `\"lon\"` | Longitude coordinate name in this dataset |\n",
    "| `y_dimension` | `\"lat\"` | Latitude coordinate name in this dataset |\n",
    "\n",
    "`extract_spatial_extent_rio` uses `ds.rio.bounds()` (already in WGS84 for this\n",
    "lat/lon dataset) to get accurate bounds, matching the rioxarray approach in the\n",
    "original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = f\"nldas-3-virtual-zarr-{SNAPSHOT_ID.lower()}\"\n",
    "\n",
    "item = build_stac_item_from_icechunk(\n",
    "    ds,\n",
    "    item_id=item_id,\n",
    "    icechunk_href=ICECHUNK_HREF,\n",
    "    snapshot_id=SNAPSHOT_ID,\n",
    "    storage_schemes=storage_schemes,\n",
    "    title=ds.attrs.get(\"title\", \"NLDAS-3 Virtual Zarr Store\"),\n",
    "    description=(\n",
    "        \"NLDAS-3 provides a fine-scale (1 km) meteorological forcing (precipitation) in \"\n",
    "        \"both retrospective and near real-time over North and Central America, including \"\n",
    "        \"Alaska, Hawaii, and Puerto Rico, by leveraging high-quality gauge, satellite, \"\n",
    "        \"and model datasets through advanced data assimilation methods. \"\n",
    "        \"Read more: https://ldas.gsfc.nasa.gov/nldas/v3\"\n",
    "    ),\n",
    "    providers=providers,\n",
    "    virtual=True,               # virtual chunk references\n",
    "    temporal_dimension=\"time\",\n",
    "    x_dimension=\"lon\",\n",
    "    y_dimension=\"lat\",\n",
    ")\n",
    "\n",
    "print(json.dumps(item, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect key fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bbox:           \", item[\"bbox\"])\n",
    "print(\"start_datetime: \", item[\"properties\"][\"start_datetime\"])\n",
    "print(\"end_datetime:   \", item[\"properties\"][\"end_datetime\"])\n",
    "print(\"variables:      \", list(item[\"properties\"][\"cube:variables\"].keys()))\n",
    "print()\n",
    "print(\"assets:\")\n",
    "for key, asset in item[\"assets\"].items():\n",
    "    print(f\"  {key}: {asset['href']}\")\n",
    "    print(f\"    snapshot_id: {asset.get('icechunk:snapshot_id')}\")\n",
    "    print(f\"    roles:       {asset['roles']}\")\n",
    "print()\n",
    "print(\"storage:schemes at top level:\", \"storage:schemes\" in item)\n",
    "print(\"storage:schemes in properties (should be False):\",\n",
    "      \"storage:schemes\" in item.get(\"properties\", {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and round-trip via xpystac\n",
    "\n",
    "Note: `xr.open_dataset(asset)` via xpystac will reconstruct the full icechunk\n",
    "config including the `VirtualChunkContainer` from `storage:schemes`.  This only\n",
    "works if `storage:schemes` is at the item top level (not in `properties`) —\n",
    "which is what `build_stac_item_from_icechunk` ensures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f\"{item_id}.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(item, f, indent=2)\n",
    "print(f\"Written to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and open via xpystac  (requires: pip install xpystac)\n",
    "loaded_item = pystac.Item.from_file(out_path)\n",
    "\n",
    "asset_key = next(k for k in loaded_item.assets if \"@\" in k)\n",
    "asset = loaded_item.assets[asset_key]\n",
    "print(f\"Opening asset: {asset_key}\")\n",
    "\n",
    "ds_from_stac = xr.open_dataset(asset)\n",
    "ds_from_stac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
